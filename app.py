import os
from tempfile import NamedTemporaryFile
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from PIL import Image
import pytesseract
from openai import OpenAI

# ‚îÄ‚îÄ‚îÄ LangChain Community & Core imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings, CohereEmbeddings, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document

# ‚îÄ‚îÄ‚îÄ Hugging Face pipeline imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.pipelines import pipeline

# ‚îÄ‚îÄ‚îÄ Load environment & configure Streamlit ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

st.set_page_config(page_title="Policy QA Agent", layout="wide")
st.title("üìÑ Policy QA Agent")

# ‚îÄ‚îÄ‚îÄ Constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
UPLOAD_LIMIT_MB = int(os.getenv("UPLOAD_LIMIT_MB", 10))
INDEX_DIR = Path(os.getenv("INDEX_DIR", "faiss_index"))

# ‚îÄ‚îÄ‚îÄ Embedding model options ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EMBED_OPTIONS = {
    "OpenAI ‚Ä¢ text-embedding-ada-002": lambda: OpenAIEmbeddings(model="text-embedding-ada-002"),
    "Cohere ‚Ä¢ embed-english-v2.0":    lambda: CohereEmbeddings(
        model="embed-english-v2.0",
        cohere_api_key=os.getenv("COHERE_API_KEY", ""),
    ),
    "HF ‚Ä¢ multi-qa-mpnet-base-dot-v1": lambda: HuggingFaceEmbeddings(
        model_name="sentence-transformers/multi-qa-mpnet-base-dot-v1"
    ),
    "HF ‚Ä¢ all-MiniLM-L12-v2":        lambda: HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L12-v2"
    ),
}

# ‚îÄ‚îÄ‚îÄ Dynamically list available OpenAI chat models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_resource(show_spinner=False)
def get_available_openai_models() -> list[str]:
    data = client.models.list().data
    return sorted(
        m.id
        for m in data
        if isinstance(m.id, str) and (m.id.startswith("gpt-") or m.id.startswith("gpt4"))
    )

OPENAI_MODELS = get_available_openai_models()

# ‚îÄ‚îÄ‚îÄ Sidebar UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.sidebar.header("‚öôÔ∏è Settings")
embed_choice = st.sidebar.selectbox("Embedding model", list(EMBED_OPTIONS.keys()), index=0)
llm_choice   = st.sidebar.selectbox("LLM model", OPENAI_MODELS + ["Local ‚Ä¢ GPT-2"], index=0)

# ‚îÄ‚îÄ‚îÄ Instantiate embedder & LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
embedder = EMBED_OPTIONS[embed_choice]()

if llm_choice == "Local ‚Ä¢ GPT-2":
    hf_pipe = pipeline(
        "text-generation",
        model=AutoModelForCausalLM.from_pretrained("gpt2"),
        tokenizer=AutoTokenizer.from_pretrained("gpt2"),
        max_new_tokens=512,
    )
    llm = HuggingFacePipeline(pipeline=hf_pipe)
else:
    llm = ChatOpenAI(model_name=llm_choice, temperature=0)

# ‚îÄ‚îÄ‚îÄ Helper functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def sanitize(name: str) -> str:
    return Path(name).stem.replace(" ", "_")

def extract_text_from_pdf(uploaded_file) -> str:
    with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp.flush()
        loader = PyPDFLoader(str(tmp.name))
        pages = loader.load()
    return "\n\n".join(page.page_content for page in pages)

def extract_text_from_image(uploaded_file) -> str:
    img = Image.open(uploaded_file)
    return pytesseract.image_to_string(img)

def validate_policy(text: str, model_name: str) -> bool:
    prompt = (
        "You are a policy validation assistant.\n"
        "Reply 'YES' if the document is a medical or health‚Äëinsurance policy, otherwise 'NO'.\n\n"
        f"DOCUMENT (first 1,500 chars):\n{text[:1500]}"
    )
    resp = client.chat.completions.create(
        model=model_name,
        messages=[{"role":"user","content":prompt}],
        temperature=0,
    )
    verdict = resp.choices[0].message.content.strip().upper()
    return verdict.startswith("Y")

@st.cache_resource
def build_vector_store(_docs: list[Document]) -> FAISS:
    if INDEX_DIR.exists():
        return FAISS.load_local(str(INDEX_DIR), embedder)
    vs = FAISS.from_documents(_docs, embedder)
    vs.save_local(str(INDEX_DIR))
    return vs

# ‚îÄ‚îÄ‚îÄ File upload & processing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
uploads = st.file_uploader(
    "Upload policy PDF or image(s)",
    type=["pdf", "png", "jpg", "jpeg"],
    accept_multiple_files=True
)

if uploads:
    full_text = ""
    for f in uploads:
        if f.size > UPLOAD_LIMIT_MB * 1024**2:
            st.error(f"{sanitize(f.name)} exceeds {UPLOAD_LIMIT_MB}¬†MB limit.")
            st.stop()
        if f.type == "application/pdf":
            full_text += extract_text_from_pdf(f)
        else:
            full_text += extract_text_from_image(f)
        full_text += "\n\n---DOC_BREAK---\n\n"

    if not validate_policy(full_text, llm_choice):
        st.error("‚ö†Ô∏è This doesn‚Äôt look like a health‚Äëinsurance policy.")
        st.stop()

    st.success("‚úÖ Policy validated. Building vector index‚Ä¶")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs     = splitter.split_documents([Document(page_content=full_text)])
    vs       = build_vector_store(docs)

    # ‚Üê use ConversationalRetrievalChain + custom retriever k
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vs.as_retriever(search_kwargs={"k": 4}),
    )

    if "history" not in st.session_state:
        st.session_state.history = []

    query = st.text_input("Ask questions about your policy:")
    if query:
        with st.spinner("üîç Thinking‚Ä¶"):
            result = qa_chain({
                "question": query,
                "chat_history": st.session_state.history
            })
        st.markdown("**Answer:**")
        st.write(result["answer"])
        st.session_state.history.append((query, result["answer"]))
