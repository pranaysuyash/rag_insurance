# PolicyÂ QAÂ Agent

**PolicyÂ QAÂ Agent** is a Streamlit application that validates, indexes and answers questions about health or medical insurance policies.Â It combines classic metadata extraction with hybrid **RAG** (Retrievalâ€‘Augmented Generation) so users can upload a policy PDF or scan and immediately start chatting with it.

---

## âœ¨ Whatâ€™s new (AprilÂ 2025)

- **Promptâ€‘fix release â€“ no more `ValueError: variable context should be a listâ€¯â€¦`**  
  The RAG prompt now injects the retrieved context as a plain string instead of a `MessagesPlaceholder`, eliminating the runtime crash.
- **InstantÂ answers from metadata** (e.g. commencement date, _insuredâ€‘since_ rows) before invoking the vector store â€“ lightningâ€‘fast for common queries.
- **Singleâ€‘file appÂ â¡Â `policy_rag_hybrid.py`** â€“ clearer naming, easier `streamlit run`.

---

## Table of Contents

1. [Features](#features)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Configuration](#configuration)
5. [Usage](#usage)
6. [FileÂ Structure](#file-structure)
7. [Dependencies](#dependencies)
8. [Contributing](#contributing)
9. [License](#license)

---

## 1Â Â Features

| Category               | Description                                                                                                             |
| ---------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Smart validation**   | Verifies the upload is a health/medical insurance document using GPT.                                                   |
| **OCRÂ +Â PDF parsing**  | Extracts text from native PDFs _or_ scanned images via `pytesseract`.                                                   |
| **Metadata fastâ€‘path** | Detects policy commencement / _insuredÂ since_ dates and answers certain questions instantly.                            |
| **HybridÂ RAG**         | Splits text + flattened table rows, embeds with OpenAIÂ Ada, stores in FAISS and feeds topâ€‘K chunks to GPTâ€‘4â€‘class LLMs. |
| **Conversational QA**  | Natural chat interface in Streamlit with sourceâ€‘chunk disclosure.                                                       |
| **Model swap**         | Dropâ€‘in support for alternative embedding or LLM providers (Cohere, HuggingÂ Face, local GGUF).                          |
| **Caching**            | Parsed text, OCR output and FAISS indexes are cached for repeat uploads.                                                |

---

## 2Â Â Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  upload    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  StreamlitÂ UI    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  PDFÂ /Â Image  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚Â Extraction    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ plainâ€‘text + rows
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Split,Â Embed (Adaâ€‘002) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   FAISSÂ VectorÂ Store   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                               questionâ”‚answer
                                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  ConversationalÂ RAG    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ rescueâ€‘LLM (optional)
                                       â–¼
                                StreamlitÂ UI
```

---

## 3Â Â Installation

```bash
# 1.Â Clone
$ git clone https://github.com/yourâ€‘repo/policyâ€‘qaâ€‘agent.git
$ cd policyâ€‘qaâ€‘agent

# 2.Â VirtualÂ env
$ python3 -m venv venv && source venv/bin/activate

# 3.Â Dependencies
$ pip install -r requirements.txt

# 4.Â Tesseract (if you need OCR)
# macOS:  brew install tesseract
# Ubuntu: sudo apt-get install tesseract-ocr
```

---

## 4Â Â Configuration

1. Copy `.env.example` âœ `.env`
2. Fill in keys / tweak limits:

```dotenv
OPENAI_API_KEY=your_openai_key
UPLOAD_LIMIT_MB=25
INDEX_DIR=faiss_hybrid
PRIMARY_MODEL=gpt-4o-mini
```

---

## 5Â Â Usage

```bash
streamlit run policy_rag_hybrid.py
```

1. Visit `http://localhost:8501`.
2. Dragâ€‘andâ€‘drop one or more policy PDFs/scans.
3. Ask questions like:
   - â€œSince when am I with NivaÂ Bupa?â€
   - â€œWhat is the waiting period for cataract surgery?â€
4. The answer panel shows the reply + expandable source snippets.

---

## 6Â Â FileÂ Structure

```text
â”œâ”€â”€ policy_rag_hybrid.py   # Main Streamlit app (this repo)
â”œâ”€â”€ requirements.txt       # Python deps
â”œâ”€â”€ .env.example           # Env template
â”œâ”€â”€ faiss_hybrid/          # Persisted FAISS indexes (autoâ€‘created)
â””â”€â”€ README.md              # You are here
```

---

## 7Â Â Dependencies (pip)

- **streamlit**Â / **pythonâ€‘dotenv**
- **openai**Â (&Â `langchain-openai`)
- **langchainâ€‘community**
- **faissâ€‘cpu**
- **pypdf**, **pdfplumber**, **pdf2image**
- **pillow**, **pytesseract**
- **transformers**, **torch**Â (optional local LLMs)
- **cohere**Â (optional embeddings)

---

## 8Â Â Contributing

```bash
git checkout -b feat/my-awesomeâ€‘thing
# hackâ€¦
git commit -m "feat: add my awesome thing"
git push origin feat/my-awesomeâ€‘thing
```

Then open a Pull Request ğŸ‰. Please match the existing code style and include tests if the feature is logicâ€‘heavy.

---

## 9Â Â License

[MIT](LICENSE)
